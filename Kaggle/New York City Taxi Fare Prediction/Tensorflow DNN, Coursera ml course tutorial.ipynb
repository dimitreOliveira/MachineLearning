{
  "cells": [
    {
      "metadata": {
        "_uuid": "a270cb5e610baebc5967ae2f121684e27614eaba"
      },
      "cell_type": "markdown",
      "source": "<center><h2>Taxi fare predictions with deep learning and Tensorflow</h2></center>\n\n### In this version of the code i use pandas to load the data and the Tensorflow input pandas function to feed the model.\n\n#### Notes: \n* [Link for a Keras version](https://www.kaggle.com/dimitreoliveira/taxi-fare-prediction-with-keras-deep-learning)\n* [Link for a more complete version on Github](https://github.com/dimitreOliveira/NewYorkCityTaxiFare)\n* I'm not using \"passenger count\" because it something that is not supposed to really matter in this case.\n* I've created two features derived from \"hour\" (night and late night), according to some research i did it's added an additional value if it's a business day (mon ~ fri) and it's night, also there's another added value if it's dawn (late night).\n* I'm binning latitudes and longitudes to make it easier to work with.\n* Even tough deep learning is robust enough to deal with noisy data, i'm removing outliers (it may save some memory).\n* Currently i'm using both Euclidean and Manhattan distances, it may be a bit redundant, but they have a different meaning and i'm still not sure of witch one is better(if you have some insights about this please let me know)"
    },
    {
      "metadata": {
        "_uuid": "7b3585dc35869d759757b1aca29203d049d3e427"
      },
      "cell_type": "markdown",
      "source": "## Dependencies"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom datetime import datetime\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n# logging in INFO mode let us see the training feedback.\ntf.logging.set_verbosity(tf.logging.INFO)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "baa8f73f9ef5da7e7cf276f0464b8572d8af610e"
      },
      "cell_type": "markdown",
      "source": "## Data clean\n### Here i'm removing some outliers, and noisy data.\n* Lats and lons that do not belong to New York.\n* Negative fare.\n* Fare greater than 250 (this seems to be noisy data).\n* Rides that begin and end in the same location."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "644b4dd101c65c1622c535c1995e3f28e7c3f3ad"
      },
      "cell_type": "code",
      "source": "def clean(df):\n    # Delimiter lats and lons to NY only\n    df = df[(-76 <= df['pickup_longitude']) & (df['pickup_longitude'] <= -72)]\n    df = df[(-76 <= df['dropoff_longitude']) & (df['dropoff_longitude'] <= -72)]\n    df = df[(38 <= df['pickup_latitude']) & (df['pickup_latitude'] <= 42)]\n    df = df[(38 <= df['dropoff_latitude']) & (df['dropoff_latitude'] <= 42)]\n    # Remove possible outliers\n    df = df[(0 < df['fare_amount']) & (df['fare_amount'] <= 250)]\n    # Remove inconsistent values\n    df = df[(df['dropoff_longitude'] != df['pickup_longitude'])]\n    df = df[(df['dropoff_latitude'] != df['pickup_latitude'])]\n    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fb6ae443b3d72143ec3dd154f26f0dcb6e0e8787"
      },
      "cell_type": "markdown",
      "source": "## Feature engineering\n*  Now i'll do some feature engineering and process the data, i'm basically creating 3 kinds of features.\n    *  **Time features**\n        * Year, Month, Day, Hour, Weekday\n        * Night (between 16h and 20h, from monday to friday)\n        * Late night (between 20h and and 6h)\n    * **Coordinate features**\n        * Latitude difference (difference from pickup and dropout latitudes)\n        * Longitude difference (difference from pickup and dropout longitudes)\n    * **Distances features**\n        * Euclidean (Euclidean distance from pickup and dropout)\n        * Manhattan (Manhattan distance from pickup and dropout)\n        * Manhattan distances from pickup location and downtown, JFK, EWR and LGR airports (see if the ride started at one of these locations).\n        * Manhattan distances from dropout location and downtown, JFK, EWR and LGR airports (see if the ride ended at one of these locations)."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "7a7f8c07755d482f7c0baaa4e7c23afaa9785077"
      },
      "cell_type": "code",
      "source": "def late_night (row):\n    if (row['hour'] <= 6) or (row['hour'] >= 20):\n        return 1\n    else:\n        return 0\n\n\ndef night (row):\n    if ((row['hour'] <= 20) and (row['hour'] >= 16)) and (row['weekday'] < 5):\n        return 1\n    else:\n        return 0\n    \n    \ndef manhattan(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n    return np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n\n\ndef add_time_features(df):\n    df['pickup_datetime'] =  pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S %Z')\n    df['year'] = df['pickup_datetime'].apply(lambda x: x.year)\n    df['month'] = df['pickup_datetime'].apply(lambda x: x.month)\n    df['day'] = df['pickup_datetime'].apply(lambda x: x.day)\n    df['hour'] = df['pickup_datetime'].apply(lambda x: x.hour)\n    df['weekday'] = df['pickup_datetime'].apply(lambda x: x.weekday())\n    df['pickup_datetime'] =  df['pickup_datetime'].apply(lambda x: str(x))\n    df['night'] = df.apply (lambda x: night(x), axis=1)\n    df['late_night'] = df.apply (lambda x: late_night(x), axis=1)\n    # Drop 'pickup_datetime' as we won't need it anymore\n    df = df.drop('pickup_datetime', axis=1)\n    \n    return df\n\n\ndef add_coordinate_features(df):\n    lat1 = df['pickup_latitude']\n    lat2 = df['dropoff_latitude']\n    lon1 = df['pickup_longitude']\n    lon2 = df['dropoff_longitude']\n    \n    # Add new features\n    df['latdiff'] = (lat1 - lat2)\n    df['londiff'] = (lon1 - lon2)\n\n    return df\n\n\ndef add_distances_features(df):\n    # Add distances from airpot and downtown\n    ny = (-74.0063889, 40.7141667)\n    jfk = (-73.7822222222, 40.6441666667)\n    ewr = (-74.175, 40.69)\n    lgr = (-73.87, 40.77)\n    \n    lat1 = df['pickup_latitude']\n    lat2 = df['dropoff_latitude']\n    lon1 = df['pickup_longitude']\n    lon2 = df['dropoff_longitude']\n    \n    df['euclidean'] = (df['latdiff'] ** 2 + df['londiff'] ** 2) ** 0.5\n    df['manhattan'] = manhattan(lat1, lon1, lat2, lon2)\n    \n    df['downtown_pickup_distance'] = manhattan(ny[1], ny[0], lat1, lon1)\n    df['downtown_dropoff_distance'] = manhattan(ny[1], ny[0], lat2, lon2)\n    df['jfk_pickup_distance'] = manhattan(jfk[1], jfk[0], lat1, lon1)\n    df['jfk_dropoff_distance'] = manhattan(jfk[1], jfk[0], lat2, lon2)\n    df['ewr_pickup_distance'] = manhattan(ewr[1], ewr[0], lat1, lon1)\n    df['ewr_dropoff_distance'] = manhattan(ewr[1], ewr[0], lat2, lon2)\n    df['lgr_pickup_distance'] = manhattan(lgr[1], lgr[0], lat1, lon1)\n    df['lgr_dropoff_distance'] = manhattan(lgr[1], lgr[0], lat2, lon2)\n    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7e09cca624e846d6d51f92d68bb0a7910151724b"
      },
      "cell_type": "markdown",
      "source": "## Estimator\n#### Here is where most of the important Tensorflow stuff happens. \n* I get a list of input columns, theses columns comes from the csv data, so they are supposed to match each other in size.\n* First we need a lists with the buckets for the latitudes and longitudes values, we give the limits and number of buckets we want.\n* Then i'll bucktize (create bins) for the values (the number of buckets is nbuckets, which is a hyperparameter).\n* Tensorflow allows us to cross categorical data, we will do it to create ploc (pickup location) and dloc (dropout location), in practice this will get categorical features and make combinations with another categorical feature, creating all possible combinations with the columns (this probably will create a very sparse column).\nWe will create sparse features for \"pickup location\" (pickup lat with lon), \"dropout location\" (dropout lat with lon), \"pickup and dropout location\" (pickup location with dropout location) and \"day_hour ride\" (weekday with hour), these features are supposed to give the model a better understanding of the data by us giving our insights.\n* The wide columns list will go to the linear model, these are features that may have a linear relation, or are a sparse feature (Tensorflow DNN models do note accept sparse data).\n* The deep columns list will go to the DNN model\n\n#### On embeddings\n* Here is very important the embedding features, as \"pd_pair\" and \"day_hr\" are sparse data, we need a way reduce it's dimension, and we do it by using a embedding, embeddings also helps mapping features to a more \"compact form\", as we try to find a way to represent a number of features in a lower dimension, e.g. in the \"day_hr\" feature i have a cross of one-hot encoded features, weekdays (7 features [one for each day]) and hour (24 features [1 for each hour]), so crossing these two would give me an additional 168 features (24 x 7), this would be a lot of input data, it would have a chance to disrupt the model learning, but by using embeddings i can lower it's dimension (168) to a smaller number, in this case i use 10, this way during training my model will also learn a way to map this 168 features to only 10 features.\n* As you can see embeddings are a very powerful tool that Tensorflow give to us, on a very easy to use API.\n* This image may help the understanding: <img src=\"https://www.tensorflow.org/images/feature_columns/embedding_vs_indicator.jpg\" width=\"450\">\n* In this model i use both linear and DNN models, to try to take advantage on both based on the features i have, luckily Tensorflow has that already implemented on it's high level API, so i just have to call tf.estimator.DNNLinearCombinedRegressor passing all the parameters i need."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def build_estimator(nbuckets, hidden_units, optimizer, input_columns, run_config=None):\n    # Input columns\n    (plon, plat, dlon, dlat, year, month, day, hour, weekday, night, late_night, \n     latdiff, londiff, euclidean, manhattan, downtown_pickup_distance, downtown_dropoff_distance, \n     jfk_pickup_distance, jfk_dropoff_distance, ewr_pickup_distance, ewr_dropoff_distance, \n     lgr_pickup_distance, lgr_dropoff_distance) = input_columns\n\n    # Bucketize the lats & lons\n    latbuckets = np.linspace(38.0, 42.0, nbuckets).tolist()\n    lonbuckets = np.linspace(-76.0, -72.0, nbuckets).tolist()\n    b_plat = tf.feature_column.bucketized_column(plat, latbuckets)\n    b_dlat = tf.feature_column.bucketized_column(dlat, latbuckets)\n    b_plon = tf.feature_column.bucketized_column(plon, lonbuckets)\n    b_dlon = tf.feature_column.bucketized_column(dlon, lonbuckets)\n\n    # Feature cross\n    ploc = tf.feature_column.crossed_column([b_plat, b_plon], nbuckets ** 2)\n    dloc = tf.feature_column.crossed_column([b_dlat, b_dlon], nbuckets ** 2)\n    pd_pair = tf.feature_column.crossed_column([ploc, dloc], nbuckets ** 4)\n    day_hr = tf.feature_column.crossed_column([weekday, hour], 24 * 7)\n\n    # Wide columns and deep columns\n    wide_columns = [\n        # Sparse columns\n        night, late_night,\n\n        # Anything with a linear relationship\n        month, hour, weekday, year\n    ]\n\n    deep_columns = [\n        # Embedding columns to \"group\" together\n        tf.feature_column.embedding_column(pd_pair, nbuckets),\n        tf.feature_column.embedding_column(day_hr, nbuckets),\n        tf.feature_column.embedding_column(ploc, nbuckets),\n        tf.feature_column.embedding_column(dloc, nbuckets),\n\n        # Numeric columns\n        latdiff, londiff,\n        euclidean, manhattan,\n        downtown_pickup_distance, downtown_dropoff_distance,\n        jfk_pickup_distance, jfk_dropoff_distance,\n        ewr_pickup_distance, ewr_dropoff_distance,\n        lgr_pickup_distance, lgr_dropoff_distance\n    ]\n\n    estimator = tf.estimator.DNNLinearCombinedRegressor(\n        linear_feature_columns=wide_columns,\n        dnn_feature_columns=deep_columns,\n        dnn_hidden_units=hidden_units,\n        dnn_optimizer=optimizer,\n        config=run_config)\n\n    return estimator",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3bf18ff64b3945ede39113e20b478570d61431bd"
      },
      "cell_type": "markdown",
      "source": "### Tensorflow pandas data input functions\n#### These functions are responsible for feeding data both for model training and prediction.\n* Note: both functions feed data by batch using generators.\n* Shuffle in the test function must be 'False' or else it will shuffle the predicted data."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "0044e0736c6775419479340b4d4c82f6b412428e"
      },
      "cell_type": "code",
      "source": "def pandas_train_input_fn(df, label):\n    return tf.estimator.inputs.pandas_input_fn(\n        x=df,\n        y=label,\n        batch_size=128,\n        num_epochs=100,\n        shuffle=True,\n        queue_capacity=1000\n    )\n\n\ndef pandas_test_input_fn(df):\n    return tf.estimator.inputs.pandas_input_fn(\n        x=df,\n        y=None,\n        batch_size=128,\n        num_epochs=1,\n        shuffle=False,\n        queue_capacity=1000\n    )",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "670dbc4b25e082a202de19441e0a612c3817e16e"
      },
      "cell_type": "markdown",
      "source": "### Output function"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "216cd5e7c1d0c77fa7512fcd20586bf11f6dc100"
      },
      "cell_type": "code",
      "source": "def output_submission(df, prediction_df, id_column, prediction_column, file_name):\n    df[prediction_column] = prediction_df['predictions'].apply(lambda x: x[0])\n    df[[id_column, prediction_column]].to_csv((file_name), index=False)\n    print('Output complete')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "500d40d4fd892533be8ec0ee28623c657246f4c6"
      },
      "cell_type": "markdown",
      "source": "### Parameters\n* One thing this API of Tensorflow have different from other ml models is that it does not have an epochs number, this API uses number of steps, this is because Tensorflow can be also used for distributed training, so it's easier to distribute training if clusters don't need do synchronize epochs with each other, with steps they simply train x steps and send the results back to the main core.\n    * Each step feeds the model with data equal to the batch size."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f30fdeaf7bf8d83fef7829eeaa6011f8726447ea",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "TRAIN_PATH = '../input/train.csv'\nTEST_PATH = '../input/test.csv'\nSUBMISSION_NAME = 'submission.csv'\n\n# Model parameters\nBATCH_SIZE = 512\nSTEPS = 400000\nLEARNING_RATE = 0.001\nDATASET_SIZE = 8000000\nHIDDEN_UNITS = [256, 128, 64, 32]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "673f993815120ec2d08c73c476e418202bb39434"
      },
      "cell_type": "markdown",
      "source": "#### Inputting columns to the Tensorflow model\n* This list is basically the columns read from the csv file converted to a Tensorflow feature column type, in this case i use only 2, but there are [many more.](https://www.tensorflow.org/guide/feature_columns)\n    * tf.feature_column.numeric_column, just a normal numeric feature.\n    * tf.feature_column.categorical_column_with_identity, this one is a categorical feature with a number of buckets (it's essentially a one-hot encoded column, but the number of buckets must be >= than the number of possible values, e.g. for week day i need at least 7 buckets)."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1f910d308c4fcabfa98bddaa4bb2bc1d9f14646",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "INPUT_COLUMNS = [\n    # raw data columns\n    tf.feature_column.numeric_column('pickup_longitude'),\n    tf.feature_column.numeric_column('pickup_latitude'),\n    tf.feature_column.numeric_column('dropoff_longitude'),\n    tf.feature_column.numeric_column('dropoff_latitude'),\n\n    # engineered columns\n    tf.feature_column.numeric_column('year'),\n    tf.feature_column.categorical_column_with_identity('month', num_buckets=13),\n    tf.feature_column.categorical_column_with_identity('day', num_buckets=32),\n    tf.feature_column.categorical_column_with_identity('hour', num_buckets=24),\n    tf.feature_column.categorical_column_with_identity('weekday', num_buckets=7),\n    tf.feature_column.categorical_column_with_identity('night', num_buckets=2),\n    tf.feature_column.categorical_column_with_identity('late_night', num_buckets=2),\n    tf.feature_column.numeric_column('latdiff'),\n    tf.feature_column.numeric_column('londiff'),\n    tf.feature_column.numeric_column('euclidean'),\n    tf.feature_column.numeric_column('manhattan'),\n    tf.feature_column.numeric_column('downtown_pickup_distance'),\n    tf.feature_column.numeric_column('downtown_dropoff_distance'),\n    tf.feature_column.numeric_column('jfk_pickup_distance'),\n    tf.feature_column.numeric_column('jfk_dropoff_distance'),\n    tf.feature_column.numeric_column('ewr_pickup_distance'),\n    tf.feature_column.numeric_column('ewr_dropoff_distance'),\n    tf.feature_column.numeric_column('lgr_pickup_distance'),\n    tf.feature_column.numeric_column('lgr_dropoff_distance')\n]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bd88e35d7122c9d8b350f724569eefb27dcef946"
      },
      "cell_type": "markdown",
      "source": "### Load data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "68bc59b570b11a83eb69596c765f4aca3e34107c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Load data in a more compact form\ndatatypes = {'key': 'str', \n              'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n\n# Only a fraction of the data\ntrain = pd.read_csv(TRAIN_PATH, nrows=DATASET_SIZE, dtype=datatypes, usecols=[1,2,3,4,5,6])\ntest = pd.read_csv(TEST_PATH)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2bdeaf171eab70008a20611018416ab99a399e29"
      },
      "cell_type": "markdown",
      "source": "#### Clean and process data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42019bd0ec9a6dae4cfaae55c0b10b8c52f2473a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train = clean(train)\n\ntrain = add_time_features(train)\ntest = add_time_features(test)\n\nadd_coordinate_features(train)\nadd_coordinate_features(test)\n\ntrain = add_distances_features(train)\ntest = add_distances_features(test)\n\ntrain.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0fb237937390bdfa0f730e806ec9cd026736bca0"
      },
      "cell_type": "markdown",
      "source": "#### Split data in train and validation (90% ~ 10%)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7643b62ebea2957c057788556806129fd38c83eb",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_df, validation_df = train_test_split(train, test_size=0.1, random_state=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "_uuid": "df46d50f55e2e12595bcfe177f68c9bf0519d251",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Scale data\n# Note: i'm doing this here with sklearn scaler but on the Coursera code the scaling is done with Dataflow and Tensorflow\n# Selecting only columns that will be scaled\nwanted_columns = ['pickup_longitude', 'pickup_latitude','dropoff_longitude', \n                  'dropoff_latitude','year', 'latdiff', 'londiff', \n                  'euclidean', 'manhattan', 'downtown_pickup_distance', \n                  'downtown_dropoff_distance', 'jfk_pickup_distance', 'jfk_dropoff_distance', \n                  'ewr_pickup_distance', 'ewr_dropoff_distance', 'lgr_pickup_distance', \n                  'lgr_dropoff_distance']\n\n# One-hot encodded features (e.g. weekday) won't be scaled, this is arguable, but in my opinion when you scale one-hot encoded features they may lose it's purpose (true or false).\none_hot_columns = ['month', 'day', 'hour', 'weekday', 'night', 'late_night']\n\ntrain_df_scaled = train_df[wanted_columns]\nvalidation_df_scaled = validation_df[wanted_columns]\ntest_scaled = test[wanted_columns]\n\n# Normalize using Min-Max scaling\n# Just a quick note: i use the same object to fit and transform all the data sets, because data should be normalized using a single data set(distribution) as parameter.\nscaler = preprocessing.MinMaxScaler()\ntrain_df_scaled[wanted_columns] = scaler.fit_transform(train_df_scaled[wanted_columns])\nvalidation_df_scaled[wanted_columns] = scaler.transform(validation_df_scaled[wanted_columns])\ntest_scaled[wanted_columns] = scaler.transform(test_scaled[wanted_columns])\n\n# Add one-hot encoded features\ntrain_df_scaled[one_hot_columns] = train_df[one_hot_columns]\nvalidation_df_scaled[one_hot_columns] = validation_df[one_hot_columns]\ntest_scaled[one_hot_columns] = test[one_hot_columns]\n\ntrain_df_scaled.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5f894b74dec8c70b350676381f80105d2cd37944"
      },
      "cell_type": "markdown",
      "source": "### Define model and parameters\n* Currently i'm using Adam optimizer, it seems to be the better default optimizer for DNN.\n* Tensorflow estimator API accepts train and evaluation spec (Specifications), that are classes with some of the information needed for the model training and evaluation, like the input function, labels and many more."
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "_uuid": "4d4bc4b67442c60de575c0c76da5abef13ec557a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\noptimizer = tf.train.ProximalAdagradOptimizer(learning_rate=0.1, l1_regularization_strength=0.001, l2_regularization_strength=0.001)\nestimator = build_estimator(16, HIDDEN_UNITS, optimizer, INPUT_COLUMNS)\n\ntrain_spec = tf.estimator.TrainSpec(input_fn=pandas_train_input_fn(train_df_scaled, train_df['fare_amount']), max_steps=STEPS)\neval_spec = tf.estimator.EvalSpec(input_fn=pandas_train_input_fn(validation_df_scaled, validation_df['fare_amount']), steps=500, throttle_secs=300)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "53aeb80ac7cecb61bd9fb356417e3e9013296c11"
      },
      "cell_type": "markdown",
      "source": "### Model parameters"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "186bd1513e1c7d0f034fe81ed5f197027f3e22b2",
        "scrolled": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "print('Dataset size: %s' % DATASET_SIZE)\nprint('Steps: %s' % STEPS)\nprint('Learning rate: %s' % LEARNING_RATE)\nprint('Batch size: %s' % BATCH_SIZE)\nprint('Input dimension: %s' % train_df_scaled.shape[1])\nprint('Features used: %s' % train_df.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2b38b9f3dd4ef5a740e693e41c7b8a67afcab868"
      },
      "cell_type": "markdown",
      "source": "### Train model\n* I'm training using the 'train_and_evaluate' function that allows me to train and evaluate the model at the same time."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a30ee6001cb8e03cd91251c216339a4548edea6",
        "_kg_hide-output": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "tf.estimator.train_and_evaluate(estimator, train_spec=train_spec, eval_spec=eval_spec)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "190b39ab2c9e2a15e975fa1fc8163ebb73563ad7",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Make prediction\nprediction = estimator.predict(pandas_test_input_fn(test_scaled))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ad5c4a2a913def2ed88ffafc5abf6751de10ed0f",
        "_kg_hide-output": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# output prediction\nprediction_df = pd.DataFrame(prediction)\noutput_submission(test, prediction_df, 'key', 'fare_amount', SUBMISSION_NAME)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}