{
  "cells": [
    {
      "metadata": {
        "_uuid": "3d7ff0f55d73bdf321e790575ef09eb0c27ae94b"
      },
      "cell_type": "markdown",
      "source": "### This is an adaptation of the Coursera code (originally built with tensorflow), on this one i used Keras to make the model part simpler, maybe it can help people that are new to deep learning or anyone else that want to use Keras on this competition.\n\n#### Notes: \n* [Link for a tensorflow version](https://www.kaggle.com/dimitreoliveira/tensorflow-dnn-coursera-ml-course-code)\n* [Link for a more complete version on Github](https://github.com/dimitreOliveira/NewYorkCityTaxiFare)\n* I'm not using \"passenger count\" because it something that is not supposed to really matter in this case.\n* I've created two features derived from \"hour\" (night and late night), according to some research i did it's added an additional value if it's a business day (mon ~ fri), and it's night, also there's another added value if it's dawn.\n* I'm binning latitudes and longitudes to make it easier to work with.\n* Even tough deep learning is robust enough to deal with noisy data, i'm removing outliers (it may save some memory).\n* Currently i'm using both Euclidean and Manhattan distances, it may be a bit redundant, but they have a different meaning and i'm still not sure of witch one is better(if you have some insights about this please let me know)"
    },
    {
      "metadata": {
        "_uuid": "202e10c43907e5fdc7264a2c328aa0a53ed73a10"
      },
      "cell_type": "markdown",
      "source": "### Dependencies"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-output": true,
        "collapsed": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.callbacks import EarlyStopping\nfrom keras import optimizers\nfrom keras import regularizers",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e2f2e006756ff46df570d539cc3976520d530cb6"
      },
      "cell_type": "markdown",
      "source": "## Data clean\n### Here i'm removing some outliers, and noisy data.\n* Lats and lons that do not belong to New York.\n* Negative fare.\n* Fare greater than 250 (this seems to be noisy data).\n* Rides that begin and end in the same location."
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "716e84668e42e06342b58423ea1f64a319d05f62"
      },
      "cell_type": "code",
      "source": "def clean(df):\n    # Delimiter lats and lons to NY only\n    df = df[(-76 <= df['pickup_longitude']) & (df['pickup_longitude'] <= -72)]\n    df = df[(-76 <= df['dropoff_longitude']) & (df['dropoff_longitude'] <= -72)]\n    df = df[(38 <= df['pickup_latitude']) & (df['pickup_latitude'] <= 42)]\n    df = df[(38 <= df['dropoff_latitude']) & (df['dropoff_latitude'] <= 42)]\n    # Remove possible outliers\n    df = df[(0 < df['fare_amount']) & (df['fare_amount'] <= 250)]\n    # Remove inconsistent values\n    df = df[(df['dropoff_longitude'] != df['pickup_longitude'])]\n    df = df[(df['dropoff_latitude'] != df['pickup_latitude'])]\n    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "12bb5164c954464788a0b26afb62d13f8a5f868c"
      },
      "cell_type": "markdown",
      "source": "## Feature engineering\n*  Now i'll do some feature engineering and process the data, i'm basically creating 3 kinds of features.\n    *  **Time features**\n        * Year, Month, Day, Hour, Weekday\n        * Night (between 16h and 20h, from monday to friday)\n        * Late night (between 20h and and 6h)\n    * **Coordinate features**\n        * Latitude difference (difference from pickup and dropout latitudes)\n        * Longitude difference (difference from pickup and dropout longitudes)\n    * **Distances features**\n        * Euclidean (Euclidean distance from pickup and dropout)\n        * Manhattan (Manhattan distance from pickup and dropout)\n        * Manhattan distances from pickup location and downtown, JFK, EWR and LGR airports (see if the ride started at one of these locations).\n        * Manhattan distances from dropout location and downtown, JFK, EWR and LGR airports (see if the ride ended at one of these locations)."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def late_night (row):\n    if (row['hour'] <= 6) or (row['hour'] >= 20):\n        return 1\n    else:\n        return 0\n\n\ndef night (row):\n    if ((row['hour'] <= 20) and (row['hour'] >= 16)) and (row['weekday'] < 5):\n        return 1\n    else:\n        return 0\n    \n    \ndef manhattan(pickup_lat, pickup_long, dropoff_lat, dropoff_long):\n    return np.abs(dropoff_lat - pickup_lat) + np.abs(dropoff_long - pickup_long)\n\n\ndef add_time_features(df):\n    df['pickup_datetime'] =  pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S %Z')\n    df['year'] = df['pickup_datetime'].apply(lambda x: x.year)\n    df['month'] = df['pickup_datetime'].apply(lambda x: x.month)\n    df['day'] = df['pickup_datetime'].apply(lambda x: x.day)\n    df['hour'] = df['pickup_datetime'].apply(lambda x: x.hour)\n    df['weekday'] = df['pickup_datetime'].apply(lambda x: x.weekday())\n    df['pickup_datetime'] =  df['pickup_datetime'].apply(lambda x: str(x))\n    df['night'] = df.apply (lambda x: night(x), axis=1)\n    df['late_night'] = df.apply (lambda x: late_night(x), axis=1)\n    # Drop 'pickup_datetime' as we won't need it anymore\n    df = df.drop('pickup_datetime', axis=1)\n    \n    return df\n\n\ndef add_coordinate_features(df):\n    lat1 = df['pickup_latitude']\n    lat2 = df['dropoff_latitude']\n    lon1 = df['pickup_longitude']\n    lon2 = df['dropoff_longitude']\n    \n    # Add new features\n    df['latdiff'] = (lat1 - lat2)\n    df['londiff'] = (lon1 - lon2)\n\n    return df\n\n\ndef add_distances_features(df):\n    # Add distances from airpot and downtown\n    ny = (-74.0063889, 40.7141667)\n    jfk = (-73.7822222222, 40.6441666667)\n    ewr = (-74.175, 40.69)\n    lgr = (-73.87, 40.77)\n    \n    lat1 = df['pickup_latitude']\n    lat2 = df['dropoff_latitude']\n    lon1 = df['pickup_longitude']\n    lon2 = df['dropoff_longitude']\n    \n    df['euclidean'] = (df['latdiff'] ** 2 + df['londiff'] ** 2) ** 0.5\n    df['manhattan'] = manhattan(lat1, lon1, lat2, lon2)\n    \n    df['downtown_pickup_distance'] = manhattan(ny[1], ny[0], lat1, lon1)\n    df['downtown_dropoff_distance'] = manhattan(ny[1], ny[0], lat2, lon2)\n    df['jfk_pickup_distance'] = manhattan(jfk[1], jfk[0], lat1, lon1)\n    df['jfk_dropoff_distance'] = manhattan(jfk[1], jfk[0], lat2, lon2)\n    df['ewr_pickup_distance'] = manhattan(ewr[1], ewr[0], lat1, lon1)\n    df['ewr_dropoff_distance'] = manhattan(ewr[1], ewr[0], lat2, lon2)\n    df['lgr_pickup_distance'] = manhattan(lgr[1], lgr[0], lat1, lon1)\n    df['lgr_dropoff_distance'] = manhattan(lgr[1], lgr[0], lat2, lon2)\n    \n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a098dcf43d5c5f8bbe1dbd6e89f797160f8173d5"
      },
      "cell_type": "markdown",
      "source": "### Auxiliar functions"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "646281e5cb269122cfe005191271d57bdcf42a96",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def output_submission(raw_test, prediction, id_column, prediction_column, file_name):\n    df = pd.DataFrame(prediction, columns=[prediction_column])\n    df[id_column] = raw_test[id_column]\n    df[[id_column, prediction_column]].to_csv((file_name), index=False)\n    print('Output complete')\n    \n    \ndef plot_loss_accuracy(history):\n    plt.figure(figsize=(20,10))\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper right')\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6f26c4a260b408ffbd1e881d205eb7145a1e37ce"
      },
      "cell_type": "markdown",
      "source": "### Parameters"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a67bbf9dc5b0caab8d964837c072b229b6224a5a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "TRAIN_PATH = '../input/train.csv'\nTEST_PATH = '../input/test.csv'\nSUBMISSION_NAME = 'submission.csv'\n\n# Model parameters\nBATCH_SIZE = 256\nEPOCHS = 50\nLEARNING_RATE = 0.001\nDATASET_SIZE = 6000000",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d6e4c15bf38a9f2606d0fb2f65f152eaee7912df"
      },
      "cell_type": "markdown",
      "source": "### Load data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "99506a559b162562260d3e347558249c0ed85b7f",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Load values in a more compact form\ndatatypes = {'key': 'str', \n              'fare_amount': 'float32',\n              'pickup_datetime': 'str', \n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'uint8'}\n\n# Only a fraction of the whole data\ntrain = pd.read_csv(TRAIN_PATH, nrows=DATASET_SIZE, dtype=datatypes, usecols=[1,2,3,4,5,6])\ntest = pd.read_csv(TEST_PATH)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f7c8d4f7d38887e62deb95d0c5be74975ab22f0"
      },
      "cell_type": "markdown",
      "source": "#### Clean and process data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "061247c80aa6f3a443a59bd7736de33985cd1a7b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train = clean(train)\n\ntrain = add_time_features(train)\ntest = add_time_features(test)\n\nadd_coordinate_features(train)\nadd_coordinate_features(test)\n\ntrain = add_distances_features(train)\ntest = add_distances_features(test)\n\ntrain.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b06b6025ec3822623a120c06170e74161f0f7262",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Drop unwanted columns\ndropped_columns = ['pickup_longitude', 'pickup_latitude', \n                   'dropoff_longitude', 'dropoff_latitude']\ntrain_clean = train.drop(dropped_columns, axis=1)\ntest_clean = test.drop(dropped_columns + ['key', 'passenger_count'], axis=1)\n\n# peek data\ntrain_clean.head(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "736872d7fb45a0b3e7532db4da69da29ac4761fd"
      },
      "cell_type": "markdown",
      "source": "#### Split data in train and validation (90% ~ 10%)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ea58d88d113808dc224e176db2bc37f6c93e08db",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_df, validation_df = train_test_split(train_clean, test_size=0.10, random_state=1)\n\n# Get labels\ntrain_labels = train_df['fare_amount'].values\nvalidation_labels = validation_df['fare_amount'].values\ntrain_df = train_df.drop(['fare_amount'], axis=1)\nvalidation_df = validation_df.drop(['fare_amount'], axis=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9500aaf4067e9c953d045421f767e8a536a0efad",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Scale data\n# Note: im doing this here with sklearn scaler but, on the Coursera code the scaling is done with Dataflow and Tensorflow\nscaler = preprocessing.MinMaxScaler()\ntrain_df_scaled = scaler.fit_transform(train_df)\nvalidation_df_scaled = scaler.transform(validation_df)\ntest_scaled = scaler.transform(test_clean)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0ccc5e6695f687e26444532b1c53849e894222ad"
      },
      "cell_type": "markdown",
      "source": "### Model"
    },
    {
      "metadata": {
        "_kg_hide-output": true,
        "trusted": true,
        "_uuid": "8dd2826039f92c6d46fde06c9c030ff76bb5933f",
        "scrolled": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model = Sequential()\nmodel.add(Dense(256, activation='relu', input_dim=train_df_scaled.shape[1], activity_regularizer=regularizers.l1(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(8, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1))\n\nadam = optimizers.adam(lr=LEARNING_RATE)\nmodel.compile(loss='mse', optimizer=adam, metrics=['mae'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bfe2b169b769e5e887523c4b5b00b8320599ffb3"
      },
      "cell_type": "markdown",
      "source": "### Model parameters"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b8ba37654ff52b8d7d615090ba5fa60c85f322cf",
        "collapsed": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "print('Dataset size: %s' % DATASET_SIZE)\nprint('Epochs: %s' % EPOCHS)\nprint('Learning rate: %s' % LEARNING_RATE)\nprint('Batch size: %s' % BATCH_SIZE)\nprint('Input dimension: %s' % train_df_scaled.shape[1])\nprint('Features used: %s' % train_df.columns)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "25f481043932ddb2e57a0b7199bcb42d4a11c44e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "14ddb39832ed9661b987abaed3a3a0d4397a1fe2"
      },
      "cell_type": "markdown",
      "source": "### Train model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e0d16fe6d7fcc97e819e2c1168c38565ef34267e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "history = model.fit(x=train_df_scaled, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, \n                    verbose=1, validation_data=(validation_df_scaled, validation_labels), \n                    shuffle=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "034ee7cc55876cb8fbb4d030b988074b68ff5c97"
      },
      "cell_type": "markdown",
      "source": "### Plot metrics"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bba904fee24ae03e106a55e77ba6e43943c35b9a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plot_loss_accuracy(history)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb1f3931f926c01a966f17940fc134f1800ca39c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Make prediction\nprediction = model.predict(test_scaled, batch_size=128, verbose=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "faca9351377875f0dbfc36306c5470d3e95e313c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# output prediction\noutput_submission(test, prediction, 'key', 'fare_amount', SUBMISSION_NAME)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}