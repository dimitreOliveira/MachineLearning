{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
      },
      "cell_type": "markdown",
      "source": "# Titanic  - Deep Learning aproach\n\n### obs: this code is from my github(https://github.com/dimitreOliveira/titanicDeepLearning) that's why it's so modular"
    },
    {
      "metadata": {
        "_cell_guid": "e50446f9-e4ab-4620-ba3c-002515bef28b",
        "_uuid": "b067e54588748e8eff884421e1b2f1fd4c739ef3"
      },
      "cell_type": "markdown",
      "source": "> ### DEPENDENCIES"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "_kg_hide-input": true
      },
      "cell_type": "code",
      "source": "import csv\nimport re\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.framework import ops\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1044469f-311b-4486-b525-f54b2ca0571e",
        "_uuid": "8abc6f203946196547df569bc5ad392cc89fa775"
      },
      "cell_type": "markdown",
      "source": "### DATASET METHODS"
    },
    {
      "metadata": {
        "_cell_guid": "f8fd27d1-d898-4b4c-a0c8-23eac35fc7f3",
        "_kg_hide-input": true,
        "_uuid": "f72fbc17509c177edcead341179cd915b5b74e77",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def load_data(train_path, test_path):\n    \"\"\"\n    method for data loading\n    :param train_path: path for the train set file\n    :param test_path: path for the test set file\n    :return: a 'pandas' array for each set\n    \"\"\"\n\n    train_data = pd.read_csv(train_path)\n    test_data = pd.read_csv(test_path)\n\n    print(\"number of training examples = \" + str(train_data.shape[0]))\n    print(\"number of test examples = \" + str(test_data.shape[0]))\n    print(\"train shape: \" + str(train_data.shape))\n    print(\"test shape: \" + str(test_data.shape))\n\n    return train_data, test_data\n\n\ndef pre_process_data(df):\n    \"\"\"\n    Perform a number of pre process functions on the data set\n    :param df: pandas data frame\n    :return: updated data frame\n    \"\"\"\n    # setting `passengerID` as Index since it wont be necessary for the analysis\n    df = df.set_index(\"PassengerId\")\n\n    # convert 'Sex' values\n    df['gender'] = df['Sex'].map({'female': 0, 'male': 1}).astype(int)\n\n    # We see that 2 passengers embarked data is missing, we fill those in as the most common Embarked value\n    df.loc[df.Embarked.isnull(), 'Embarked'] = df['Embarked'].mode()[0]\n\n    # Replace missing age values with median ages by gender\n    for gender in df['gender'].unique():\n        median_age = df[(df['gender'] == gender)].Age.median()\n        df.loc[(df['Age'].isnull()) & (df['gender'] == gender), 'Age'] = median_age\n\n    # convert 'gender' values to new columns\n    df = pd.get_dummies(df, columns=['gender'])\n\n    # convert 'Embarked' values to new columns\n    df = pd.get_dummies(df, columns=['Embarked'])\n\n    # bin Fare into five intervals with equal amount of values\n#     df['Fare-bin'] = pd.qcut(df['Fare'], 5, labels=[1, 2, 3, 4, 5]).astype(int)\n\n    # bin Age into seven intervals with equal amount of values\n    # ('baby','child','teenager','young','mid-age','over-50','senior')\n    bins = [0, 4, 12, 18, 30, 50, 65, 100]\n    age_index = (1, 2, 3, 4, 5, 6, 7)\n    df['Age-bin'] = pd.cut(df['Age'], bins, labels=age_index).astype(int)\n\n    # create a new column 'family' as a sum of 'SibSp' and 'Parch'\n    df['family'] = df['SibSp'] + df['Parch'] + 1\n    df['family'] = df['family'].map(lambda x: 4 if x > 4 else x)\n\n    # create a new column 'FTicket' as the first character of the 'Ticket'\n    df['FTicket'] = df['Ticket'].map(lambda x: x[0])\n    # combine smaller categories into one\n    df['FTicket'] = df['FTicket'].replace(['W', 'F', 'L', '5', '6', '7', '8', '9'], '4')\n    # convert 'FTicket' values to new columns\n    df = pd.get_dummies(df, columns=['FTicket'])\n\n    # get titles from the name\n    df['title'] = df.apply(lambda row: re.split('[,.]+', row['Name'])[1], axis=1)\n\n    # convert titles to values\n    df['title'] = df['title'].map({' Capt': 'Other', ' Master': 'Master', ' Mr': 'Mr', ' Don': 'Other',\n                                   ' Dona': 'Other', ' Lady': 'Other', ' Col': 'Other', ' Miss': 'Miss',\n                                   ' the Countess': 'Other', ' Dr': 'Other', ' Jonkheer': 'Other', ' Mlle': 'Other',\n                                   ' Sir': 'Other', ' Rev': 'Other', ' Ms': 'Other', ' Mme': 'Other', ' Major': 'Other',\n                                   ' Mrs': 'Mrs'})\n    # convert 'title' values to new columns\n    df = pd.get_dummies(df, columns=['title'])\n\n    df = df.drop(['Name', 'Ticket', 'Cabin', 'Sex', 'Fare', 'Age'], axis=1)\n\n    return df\n\n\ndef mini_batches(train_set, train_labels, mini_batch_size):\n    \"\"\"\n    Generate mini batches from the data set (data and labels)\n    :param train_set: data set with the examples\n    :param train_labels: data set with the labels\n    :param mini_batch_size: mini batch size\n    :return: mini batches\n    \"\"\"\n    set_size = train_set.shape[0]\n    batches = []\n    num_complete_minibatches = set_size // mini_batch_size\n\n    for k in range(0, num_complete_minibatches):\n        mini_batch_x = train_set[k * mini_batch_size: (k + 1) * mini_batch_size]\n        mini_batch_y = train_labels[k * mini_batch_size: (k + 1) * mini_batch_size]\n        mini_batch = (mini_batch_x, mini_batch_y)\n        batches.append(mini_batch)\n\n    # Handling the end case (last mini-batch < mini_batch_size)\n    if set_size % mini_batch_size != 0:\n        mini_batch_x = train_set[(set_size - (set_size % mini_batch_size)):]\n        mini_batch_y = train_labels[(set_size - (set_size % mini_batch_size)):]\n        mini_batch = (mini_batch_x, mini_batch_y)\n        batches.append(mini_batch)\n\n    return batches",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "93d7d65c-ea69-465a-9c35-8f2d95a9e52f",
        "_uuid": "b99800e097206422d590d71c9365230bd17660aa"
      },
      "cell_type": "markdown",
      "source": "### AUXILIARY MODEL METHODS"
    },
    {
      "metadata": {
        "_cell_guid": "a82a5051-a589-483d-afba-28a901dae8fd",
        "_kg_hide-input": true,
        "_uuid": "6283e5f3c1841e58788761cf818749b2bbdc2e78",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_placeholders(input_size, output_size):\n    \"\"\"\n    Creates the placeholders for the tensorflow session.\n    :param input_size: scalar, input size\n    :param output_size: scalar, output size\n    :return: X  placeholder for the data input, of shape [None, input_size] and dtype \"float\"\n    :return: Y placeholder for the input labels, of shape [None, output_size] and dtype \"float\"\n    \"\"\"\n\n    x = tf.placeholder(shape=(None, input_size), dtype=tf.float32, name=\"X\")\n    y = tf.placeholder(shape=(None, output_size), dtype=tf.float32, name=\"Y\")\n\n    return x, y\n\n\ndef forward_propagation(x, parameters, keep_prob=1.0, hidden_activation='relu'):\n    \"\"\"\n    Implement forward propagation with dropout for the [LINEAR->RELU]*(L-1)->LINEAR-> computation\n    :param x: data, pandas array of shape (input size, number of examples)\n    :param parameters: output of initialize_parameters()\n    :param keep_prob: probability to keep each node of the layer\n    :param hidden_activation: activation function of the hidden layers\n    :return: last LINEAR value\n    \"\"\"\n\n    a_dropout = x\n    n_layers = len(parameters) // 2  # number of layers in the neural network\n\n    for l in range(1, n_layers):\n        a_prev = a_dropout\n        a_dropout = linear_activation_forward(a_prev, parameters['w%s' % l], parameters['b%s' % l], hidden_activation)\n\n        if keep_prob < 1.0:\n            a_dropout = tf.nn.dropout(a_dropout, keep_prob)\n\n    al = tf.matmul(a_dropout, parameters['w%s' % n_layers]) + parameters['b%s' % n_layers]\n\n    return al\n\n\ndef linear_activation_forward(a_prev, w, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n    :param a_prev: activations from previous layer (or input data): (size of previous layer, number of examples)\n    :param w: weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    :param b: bias vector, numpy array of shape (size of the current layer, 1)\n    :param activation: the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n    :return: the output of the activation function, also called the post-activation value\n    \"\"\"\n\n    a = None\n    if activation == \"sigmoid\":\n        z = tf.matmul(a_prev, w) + b\n        a = tf.nn.sigmoid(z)\n\n    elif activation == \"relu\":\n        z = tf.matmul(a_prev, w) + b\n        a = tf.nn.relu(z)\n\n    elif activation == \"leaky relu\":\n        z = tf.matmul(a_prev, w) + b\n        a = tf.nn.leaky_relu(z)\n\n    return a\n\n\ndef initialize_parameters(layer_dims):\n    \"\"\"\n    :param layer_dims: python array (list) containing the dimensions of each layer in our network\n    :return: python dictionary containing your parameters \"w1\", \"b1\", ..., \"wn\", \"bn\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n\n    parameters = {}\n    n_layers = len(layer_dims)  # number of layers in the network\n\n    for l in range(1, n_layers):\n        parameters['w' + str(l)] = tf.get_variable('w' + str(l), [layer_dims[l - 1], layer_dims[l]],\n                                                   initializer=tf.contrib.layers.xavier_initializer())\n        parameters['b' + str(l)] = tf.get_variable('b' + str(l), [layer_dims[l]], initializer=tf.zeros_initializer())\n\n    return parameters\n\n\ndef compute_cost(z3, y):\n    \"\"\"\n    :param z3: output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n    :param y: \"true\" labels vector placeholder, same shape as Z3\n    :return: Tensor of the cost function\n    \"\"\"\n\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=z3, labels=y))\n\n    return cost\n\n\ndef predict(data, parameters):\n    \"\"\"\n    make a prediction based on a data set and parameters\n    :param data: based data set\n    :param parameters: based parameters\n    :return: array of predictions\n    \"\"\"\n\n    init = tf.global_variables_initializer()\n    with tf.Session() as sess:\n        sess.run(init)\n\n        dataset = tf.cast(tf.constant(data), tf.float32)\n        fw_prop_result = forward_propagation(dataset, parameters)\n        fw_prop_activation = tf.nn.softmax(fw_prop_result)\n        prediction = fw_prop_activation.eval()\n\n    return prediction\n\n\ndef accuracy(predictions, labels):\n    \"\"\"\n    calculate accuracy between two data sets\n    :param predictions: data set of predictions\n    :param labels: data set of labels (real values)\n    :return: percentage of correct predictions\n    \"\"\"\n\n    prediction_size = predictions.shape[0]\n    prediction_accuracy = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / prediction_size\n\n    return 100 * prediction_accuracy\n\n\ndef minibatch_accuracy(predictions, labels):\n    \"\"\"\n    calculate accuracy between two data sets\n    :param predictions: data set of predictions\n    :param labels: data set of labels (real values)\n    :return: percentage of correct predictions\n    \"\"\"\n\n    prediction_size = predictions.shape[0]\n    prediction_accuracy = np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / prediction_size\n\n    return 100 * prediction_accuracy\n\n\ndef l2_regularizer(cost, l2_beta, parameters, n_layers):\n    \"\"\"\n    Function to apply l2 regularization to the model\n    :param cost: usual cost of the model\n    :param l2_beta: beta value used for the normalization\n    :param parameters: parameters from the model (used to get weights values)\n    :param n_layers: number of layers of the model\n    :return: cost updated\n    \"\"\"\n\n    regularizer = 0\n    for i in range(1, n_layers):\n        regularizer += tf.nn.l2_loss(parameters['w%s' % i])\n\n    cost = tf.reduce_mean(cost + l2_beta * regularizer)\n\n    return cost\n\n\ndef build_submission_name(layers_dims, num_epochs, lr_decay,\n                          learning_rate, l2_beta, keep_prob, minibatch_size, num_examples):\n    \"\"\"\n    builds a string (submission file name), based on the model parameters\n    :param layers_dims: model layers dimensions\n    :param num_epochs: model number of epochs\n    :param lr_decay: model learning rate decay\n    :param learning_rate: model learning rate\n    :param l2_beta: beta used on l2 normalization\n    :param keep_prob: keep probability used on dropout normalization\n    :param minibatch_size: model mini batch size (0 to do not use mini batches)\n    :param num_examples: number of model examples (training data)\n    :return: built string\n    \"\"\"\n    submission_name = 'ly{}-epoch{}.csv' \\\n        .format(layers_dims, num_epochs)\n\n    if lr_decay != 0:\n        submission_name = 'lrdc{}-'.format(lr_decay) + submission_name\n    else:\n        submission_name = 'lr{}-'.format(learning_rate) + submission_name\n\n    if l2_beta > 0:\n        submission_name = 'l2{}-'.format(l2_beta) + submission_name\n\n    if keep_prob < 1:\n        submission_name = 'dk{}-'.format(keep_prob) + submission_name\n\n    if minibatch_size != num_examples:\n        submission_name = 'mb{}-'.format(minibatch_size) + submission_name\n\n    return submission_name\n\n\ndef plot_model_cost(train_costs, validation_costs, submission_name):\n    \"\"\"\n    :param train_costs: array with the costs from the model training\n    :param validation_costs: array with the costs from the model validation\n    :param submission_name: name of the submission (used for the plot title)\n    :return:\n    \"\"\"\n    plt.plot(np.squeeze(train_costs), label='Train cost')\n    plt.plot(np.squeeze(validation_costs), label='Validation cost')\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Model: \" + submission_name)\n    plt.legend()\n    plt.show()\n\n\ndef plot_model_accuracy(train_accuracies, validation_accuracies, submission_name):\n    \"\"\"\n    :param train_accuracies: array with the accuracies from the model training\n    :param validation_accuracies: array with the accuracies from the model validation\n    :param submission_name:  name of the submission (used for the plot title)\n    :return:\n    \"\"\"\n    plt.plot(np.squeeze(train_accuracies), label='Train accuracy')\n    plt.plot(np.squeeze(validation_accuracies), label='Validation accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Model: \" + submission_name)\n    plt.legend()\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "ab641624-bbc3-488e-82d7-290d192662a5",
        "_uuid": "d849733d5ccdd979cbcafbea07ecbf130f05b8b8"
      },
      "cell_type": "markdown",
      "source": "### MODEL"
    },
    {
      "metadata": {
        "_cell_guid": "3b17134b-a5ab-4839-bcb1-35875d0639c5",
        "_kg_hide-input": true,
        "_uuid": "84c62a026713eccdd485f89649058feb3befc2b4",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def model(train_set, train_labels, validation_set, validation_labels, layers_dims, learning_rate=0.01, num_epochs=1001,\n          print_cost=True, plot_cost=True, l2_beta=0., keep_prob=1.0, hidden_activation='relu', return_best=False,\n          minibatch_size=0, lr_decay=0, print_accuracy=True, plot_accuracy=True):\n    \"\"\"\n    :param train_set: training set\n    :param train_labels: training labels\n    :param validation_set: validation set\n    :param validation_labels: validation labels\n    :param layers_dims: array with the layer for the model\n    :param learning_rate: learning rate of the optimization\n    :param num_epochs: number of epochs of the optimization loop\n    :param print_cost: True to print the cost every 500 epochs\n    :param plot_cost: True to plot the train and validation cost\n    :param l2_beta: beta parameter for the l2 regularization\n    :param keep_prob: probability to keep each node of each hidden layer (dropout)\n    :param hidden_activation: activation function to be used on the hidden layers\n    :param return_best: True to return the highest params from all epochs\n    :param minibatch_size: size of th mini batch\n    :param lr_decay: if != 0, sets de learning rate decay on each epoch\n    :param print_accuracy: True to print the accuracy every 500 epochs\n    :param plot_accuracy: True to plot the train and validation accuracy\n    :return parameters: parameters learnt by the model. They can then be used to predict.\n    :return submission_name: name for the trained model\n    \"\"\"\n\n    ops.reset_default_graph()  # to be able to rerun the model without overwriting tf variables\n\n    input_size = layers_dims[0]\n    output_size = layers_dims[-1]\n    num_examples = train_set.shape[0]\n    n_layers = len(layers_dims)\n    train_costs = []\n    validation_costs = []\n    train_accuracies = []\n    validation_accuracies = []\n    prediction = []\n    best_iteration = [float('inf'), 0, float('-inf'), 0]\n    best_params = None\n\n    if minibatch_size == 0 or minibatch_size > num_examples:\n        minibatch_size = num_examples\n\n    num_minibatches = num_examples // minibatch_size\n\n    if num_minibatches == 0:\n        num_minibatches = 1\n\n    submission_name = build_submission_name(layers_dims, num_epochs, lr_decay, learning_rate, l2_beta, keep_prob,\n                                            minibatch_size, num_examples)\n\n    x, y = create_placeholders(input_size, output_size)\n    tf_valid_dataset = tf.cast(tf.constant(validation_set), tf.float32)\n    parameters = initialize_parameters(layers_dims)\n\n    fw_output = forward_propagation(x, parameters, keep_prob, hidden_activation)\n    train_cost = compute_cost(fw_output, y)\n    train_prediction = tf.nn.softmax(fw_output)\n\n    fw_output_valid = forward_propagation(tf_valid_dataset, parameters, 1, hidden_activation)\n    validation_cost = compute_cost(fw_output_valid, validation_labels)\n    valid_prediction = tf.nn.softmax(fw_output_valid)\n\n    if l2_beta > 0:\n        train_cost = l2_regularizer(train_cost, l2_beta, parameters, n_layers)\n        validation_cost = l2_regularizer(validation_cost, l2_beta, parameters, n_layers)\n\n    if lr_decay != 0:\n        global_step = tf.Variable(0, trainable=False)\n        learning_rate = tf.train.inverse_time_decay(learning_rate, global_step=global_step, decay_rate=lr_decay,\n                                                    decay_steps=1)\n        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(train_cost, global_step=global_step)\n    else:\n        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(train_cost)\n\n    init = tf.global_variables_initializer()\n\n    with tf.Session() as sess:\n        sess.run(init)\n\n        for epoch in range(num_epochs):\n            train_epoch_cost = 0.\n            validation_epoch_cost = 0.\n\n            minibatches = mini_batches(train_set, train_labels, minibatch_size)\n\n            for minibatch in minibatches:\n                (minibatch_X, minibatch_Y) = minibatch\n                feed_dict = {x: minibatch_X, y: minibatch_Y}\n\n                _, minibatch_train_cost, prediction, minibatch_validation_cost = sess.run(\n                    [optimizer, train_cost, train_prediction, validation_cost], feed_dict=feed_dict)\n\n                train_epoch_cost += minibatch_train_cost / num_minibatches\n                validation_epoch_cost += minibatch_validation_cost / num_minibatches\n\n            validation_accuracy = accuracy(valid_prediction.eval(), validation_labels)\n            train_accuracy = accuracy(prediction, minibatch_Y)\n\n            if print_cost is True and epoch % 500 == 0:\n                print(\"Train cost after epoch %i: %f\" % (epoch, train_epoch_cost))\n                print(\"Validation cost after epoch %i: %f\" % (epoch, validation_epoch_cost))\n\n            if print_accuracy is True and epoch % 500 == 0:\n                print('Train accuracy after epoch {}: {:.2f}'.format(epoch, train_accuracy))\n                print('Validation accuracy after epoch {}: {:.2f}'.format(epoch, validation_accuracy))\n\n            if plot_cost is True and epoch % 10 == 0:\n                train_costs.append(train_epoch_cost)\n                validation_costs.append(validation_epoch_cost)\n\n            if plot_accuracy is True and epoch % 10 == 0:\n                train_accuracies.append(train_accuracy)\n                validation_accuracies.append(validation_accuracy)\n\n            if return_best is True:\n                if validation_epoch_cost < best_iteration[0]:\n                    best_iteration[0] = validation_epoch_cost\n                    best_iteration[1] = epoch\n                    best_params = sess.run(parameters)\n                if validation_accuracy > best_iteration[2]:\n                    best_iteration[2] = validation_accuracy\n                    best_iteration[3] = epoch\n                    best_params = sess.run(parameters)\n\n        if return_best is True:\n            parameters = best_params\n        else:\n            parameters = sess.run(parameters)\n\n        print(\"Parameters have been trained, getting metrics...\")\n\n        train_accuracy = accuracy(predict(train_set, parameters), train_labels)\n        validation_accuracy = accuracy(predict(validation_set, parameters), validation_labels)\n\n        print('Train accuracy: {:.2f}'.format(train_accuracy))\n        print('Validation accuracy: {:.2f}'.format(validation_accuracy))\n        print('Lowest validation cost: {:.2f} at epoch {}'.format(best_iteration[0], best_iteration[1]))\n        print('Highest validation accuracy: {:.2f} at epoch {}'.format(best_iteration[2], best_iteration[3]))\n\n        submission_name = 'tr_acc-{:.2f}-vd_acc{:.2f}-'.format(train_accuracy, validation_accuracy) + submission_name\n\n        if return_best is True:\n            print('Lowest cost: {:.2f} at epoch {}'.format(best_iteration[0], best_iteration[1]))\n\n        if plot_cost is True:\n            plot_model_cost(train_costs, validation_costs, submission_name)\n\n        if plot_accuracy is True:\n            plot_model_accuracy(train_accuracies, validation_accuracies, submission_name)\n\n        return parameters, submission_name",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0a36d260-bf5a-4807-bfb1-373f46bdcee3",
        "_uuid": "13e65784a37903e86bda95dd162e08b6398e0d39"
      },
      "cell_type": "markdown",
      "source": "### Load data"
    },
    {
      "metadata": {
        "_cell_guid": "deebe725-d270-4f86-b43c-b0b441f639c0",
        "_uuid": "a70f7d3ae908039f2a85a5619973878a5fdfa38b",
        "trusted": true
      },
      "cell_type": "code",
      "source": "TRAIN_PATH = '../input/train.csv'\nTEST_PATH = '../input/test.csv'\n\ntrain, test = load_data(TRAIN_PATH, TEST_PATH)\n\nCLASSES = 2\ntrain_dataset_size = train.shape[0]\n# The labels need to be one-hot encoded\ntrain_raw_labels = pd.get_dummies(train.Survived).as_matrix()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7f56803396835b3e7a32aa4a5c2af83533917f2f"
      },
      "cell_type": "markdown",
      "source": "### Pre process data"
    },
    {
      "metadata": {
        "_cell_guid": "13bd6af8-d74f-4101-99b3-deca0274b634",
        "_uuid": "5e763596c61a74ebf0f2ed3ab2a0308b791e6e3f",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pre_process_data(train)\ntest = pre_process_data(test)\n\n# drop unwanted columns\ntrain_pre = train.drop(['Survived'], axis=1).as_matrix().astype(np.float)\ntest_pre = test.as_matrix().astype(np.float)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ca1aa5599f1e2b0c65aa7537d646485c859151ac"
      },
      "cell_type": "markdown",
      "source": "### Normalize data"
    },
    {
      "metadata": {
        "_cell_guid": "6c847c33-da8e-48ae-950f-1511b820cbe5",
        "_uuid": "0317f061d8f0a6282aa704264b7f3394f7933b80",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# scale values\nstandard_scaler = preprocessing.StandardScaler()\ntrain_pre = standard_scaler.fit_transform(train_pre)\ntest_pre = standard_scaler.fit_transform(test_pre)\n\n# data split\nX_train, X_valid, Y_train, Y_valid = train_test_split(train_pre, train_raw_labels, test_size=0.3, random_state=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "89356b3f7e671bbcb08f2ac1d704993d13771dc4"
      },
      "cell_type": "markdown",
      "source": "### Model parameters"
    },
    {
      "metadata": {
        "_cell_guid": "ab073113-ed42-4a29-83ac-25764b9638a1",
        "_uuid": "e1f8840d19ddd9297c671cc3e8ef9bbdd54cf692",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# hyperparameters\ninput_layer = train_pre.shape[1]\noutput_layer = 2\nnum_epochs = 10001\nlearning_rate = 0.0001\ntrain_size = 0.8\n# layers_dims = [input_layer, 256, 128, 64, output_layer]\nlayers_dims = [input_layer, 512, 128, 64, output_layer]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b68f6b671bedf3d282d698e67ffc1f92b4afe803"
      },
      "cell_type": "markdown",
      "source": "### Train model"
    },
    {
      "metadata": {
        "_cell_guid": "95e380c3-7f54-418c-bf31-e51c9d4a43d6",
        "_uuid": "e9182279533cd07dd267456c5e2edbadc7dd6f23",
        "trusted": true
      },
      "cell_type": "code",
      "source": "parameters, submission_name = model(X_train, Y_train, X_valid, Y_valid, layers_dims, num_epochs=num_epochs,\n                                    learning_rate=learning_rate, print_cost=False, plot_cost=True, l2_beta=0.1,\n                                    keep_prob=0.5, minibatch_size=0, return_best=True, print_accuracy=False,\n                                    plot_accuracy=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8a1bf1087caf6d3fdd8d916082118bd36cbccb21"
      },
      "cell_type": "markdown",
      "source": "### Make predictions"
    },
    {
      "metadata": {
        "_cell_guid": "ed2c6868-0844-457c-8243-8ad2bb7790ef",
        "_uuid": "826f649ed80eb4376e88ff26c437e2d293cf8180",
        "trusted": true
      },
      "cell_type": "code",
      "source": "final_prediction = predict(test_pre, parameters)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6506b2a1c1f2e356e17316815f4f01b8918db3c"
      },
      "cell_type": "code",
      "source": "submission = pd.DataFrame({\"PassengerId\":test.index.values})\nsubmission[\"Survived\"] = np.argmax(final_prediction, 1)\nsubmission.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}